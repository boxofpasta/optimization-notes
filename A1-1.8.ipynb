{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.8\n",
    "\n",
    "Some setup code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def get_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "    \n",
    "def get_angle(x, y):\n",
    "    cos_angle = np.dot(x, y) / (norm(x) * norm(y))\n",
    "    return np.arccos(cos_angle)\n",
    "\n",
    "# This will help us get answers for both parts of the problem.\n",
    "# V is the transposed data matrix, with shape (10, 1651).\n",
    "def print_best_pairs(V):\n",
    "    min_dist, min_angle = None, None\n",
    "    min_dist_pair = None\n",
    "    min_angle_pair = None\n",
    "    for i in range(len(V)):\n",
    "        for j in range(i + 1, len(V)):\n",
    "            dist = get_distance(V[i], V[j])\n",
    "            angle = np.abs(get_angle(V[i], V[j]))\n",
    "            if dist < min_dist or min_dist is None:\n",
    "                best_dist_pair = i, j\n",
    "                min_dist = dist\n",
    "            if angle < min_angle or min_angle is None:\n",
    "                best_angle_pair = i, j\n",
    "                min_angle = angle\n",
    "            \n",
    "    print('Lowest distance pair is: (v%d, v%d)' % (best_dist_pair))\n",
    "    print('Lowest angle pair is: (v%d, v%d)' % (best_angle_pair))\n",
    "\n",
    "data_path = os.path.join('PS01_dataSet', 'wordVecV.mat')\n",
    "data = loadmat(data_path)\n",
    "V = data['V'].T\n",
    "num_docs = len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest distance pair is: (v6, v7)\n",
      "Lowest angle pair is: (v8, v9)\n"
     ]
    }
   ],
   "source": [
    "print_best_pairs(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are not the same pair. The reason for this is probably that the vectors aren't normalized, and in this case using angle vs distance for metrics gives us different answers. This was shown in Q1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest distance pair is: (v8, v9)\n",
      "Lowest angle pair is: (v8, v9)\n"
     ]
    }
   ],
   "source": [
    "normalizer = np.sum(V, axis=1, keepdims=True)\n",
    "V_l1_normed = V / normalizer\n",
    "print_best_pairs(V_l1_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest angle difference pair is the same as part a); this is expected as all we've done is scale the vectors (i.e they still point in the same directions). What has changed is that the distance metric now agrees with the angle difference metric on the nearest neighbor.\n",
    "\n",
    "One possible reason for using this normalization would be to decrease the relative distance for documents with very similar structure but differing lengths. A contrived example would be two documents A and B, where B is just A repeated a few times. In this case normalization will help make the distance between the two 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c), d) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.51742713 1.26863624 1.51742713 ... 1.51742713 1.51742713 1.51742713]]\n",
      "Lowest distance pair is: (v8, v9)\n",
      "Lowest angle pair is: (v7, v9)\n"
     ]
    }
   ],
   "source": [
    "fdoc = np.sum(V > 0, axis=0, keepdims=True) \n",
    "tfidf_log_term = np.sqrt(np.log(num_docs / fdoc))\n",
    "print(tfidf_log_term)\n",
    "V_tfidf = V_l1_normed * tfidf_log_term\n",
    "print_best_pairs(V_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"inverse document frequency\" adjustment lowers the $f_term$ values for words that occur frequently across _all_ documents, while putting relatively more scaling on words that occur only in fewer documents. Geometrically, this means creating more separation along axes of words that occur more rarely across documents, and having the resulting vectors \"point\" more along these axes.\n",
    "\n",
    "For example, the only two documents that contain the word \"optimization\" will separate themselves more from the other documents by pointing more along the \"optimization\" axis.\n",
    "\n",
    "This scaling might be useful as it helps lower the importance of words that don't help identify a document uniquely or help our distance metric, since they occur everywhere regardless of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
