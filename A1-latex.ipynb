{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$% Define macros.$\n",
    "\n",
    "$\\newcommand{\\norm}[1]{\\Vert #1 \\Vert}$\n",
    "$\\newcommand{\\lonenorm}[1]{\\sum_{i=1}^{N}|#1_i|}$\n",
    "$\\newcommand{\\linfnorm}[1]{\\max_{i=1}^{N}|#1_i|}$\n",
    "$\\newcommand{\\innerprod}[2]{\\langle\\ #1, #2 \\rangle}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 ##\n",
    "\n",
    "Properties of a norm:\n",
    "$\\norm{x}$\n",
    "1. $\\norm{x} \\geq 0, \\norm{x} = 0 \\iff x = 0$\n",
    "2. $\\norm{x + y} \\leq \\norm{x} + \\norm{y}$ , for any $x, y \\in X$\n",
    "3. $\\norm{\\alpha x} = |{\\alpha}| \\norm{x}$\n",
    "\n",
    "### (a) Verification of each of the properties of a norm for the l1 norm: \n",
    "\n",
    "1. The l1 norm is defined as $\\lonenorm{x}$. I.e it is the sum of the absolute value of each of $x$'s elements. Each element in the sum is greater than or equal to 0, so the sum must also be greater than or equal to 0. The only way for this sum to be 0 is if each element in $x$ is 0.\n",
    "\n",
    "2. For the l1 norm, this condition reads as: $\\sum_{i=1}^{N}|x_i + y_i| \\leq \\lonenorm{x} + \\lonenorm{y}$. We will show that this inequality holds for every component $i$ in the sum (and thus holds for the entire sum). If $x_i$, $y_i$ have differing signs, then $x_i + y_i \\leq |x_i| + |y_i|$, and otherwise they are equal to each other.\n",
    "\n",
    "3. The LHS and RHS are equivalent: \n",
    "$$ \n",
    "\\begin{aligned} \\lonenorm{\\alpha x} &= \\sum_{i=1}^{N}|\\alpha||x_i| \\\\\n",
    "\\lonenorm{\\alpha x} &= |\\alpha| \\lonenorm{x} \\end{aligned} \n",
    "$$\n",
    "   Where the first step is possible due to a property of the absolute value.\n",
    "      \n",
    "### b) Verification of each of the properties of a norm for the $l_{\\infty}$ norm:\n",
    "The $l_{\\infty}$ norm is defined as $\\linfnorm{x}$.\n",
    "1. Similar to part a), as each element $x_i \\geq 0$, the maximum of the absolute value of these elements has to also be $\\geq 0$. Similarly, each element $x_i = 0$, or $x = 0$, for $l_infty$ to be 0.\n",
    "2. Consider the $l_{\\infty}$ norm of $x + y$, which is the maximum over all elements $|x_i + y_i|$. We will only look at the case where $x_i, y_i$ have the same sign, as this is the best case for the LHS. On the RHS, the $l_{\\infty}$ norm is $|x_j| + |y_k|$. Since $|x_j| \\geq |x_i|$ and $|y_k| \\geq |y_i|$, we have that the RHS is greater than or equal to the LHS.\n",
    "3. For clarity, the equation is: \n",
    "$$\n",
    "\\max_{i=1}^{N}|\\alpha x_i| = |\\alpha|\\linfnorm{x}\n",
    "$$\n",
    "This is true since all the elements $|x_i|$ have been scaled by the same nonnegative factor (the scaling can be factored out as in part a) section 3), so the index $j$ of the maximum element is the same after scaling. Therefore the value of the $l_{\\infty}$ norm has been simply scaled by $|\\alpha|$, which is the expression on the RHS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 ##\n",
    "\n",
    "### Norm Inequalities ###\n",
    "\n",
    "$n$ is the dimensionality of the vector {x}. \n",
    "For $\\norm{x}_{\\infty}$, we will use $x_{max}$, the maximum absolute value element within $x$,  to denote the result of this operation.\n",
    "We will prove each of the inequalities one by one:\n",
    "\n",
    "1. $ \\dfrac{1}{\\sqrt{n}} \\norm{x}_2 \\leq \\norm{x}_{\\infty} $: \n",
    "\n",
    "    Square both sides and multiply by $n$:\n",
    "    $$\n",
    "    \\sum_{i=1}^{n}x^{2} \\leq n x_{max}^{2} = \\sum_{i=1}^{n}x_{max}^{2}\n",
    "    $$\n",
    "    As each element in the summation in the RHS is greater than or equal to the LHS, this must be true.\n",
    "\n",
    "\n",
    "2. $ \\norm{x}_{\\infty} \\leq \\norm{x}_2 $:\n",
    "\n",
    "    Square both sides and rewrite the RHS as $x_{max}^{2}$ summed with some \"residuals\":\n",
    "    $$\n",
    "    x_{max}^{2} \\leq x_{max}^{2} + R\n",
    "    $$\n",
    "    $R$, the residual, is the summation of all terms other than the maximum, and is greater than or equal to 0. Thus, this inequality must hold.\n",
    "\n",
    "\n",
    "3. $ \\norm{x}_2 \\leq \\norm{x}_1 $:\n",
    "\n",
    "    Squaring both sides, we have: \n",
    "\n",
    "    $\\sum_{i=1}^{N}|x_i|^{2} \\leq (\\lonenorm{x})^{2}$\n",
    "\n",
    "    This must be true since after expanding the RHS, we have all the terms in the LHS along with a nonnegative residual that is the sum of all the cross multiplication terms (as each of these terms is nonnegative).\n",
    "\n",
    "\n",
    "4. $ \\norm{x}_1 \\leq \\sqrt{n}\\norm{x}_2$:\n",
    "\n",
    "    Once again, square both sides:\n",
    "\n",
    "    $ (\\lonenorm{x})^{2} \\leq n (\\sum_{i=1}^{n}|x_i|^{2}) $ \n",
    "\n",
    "    This is actually a special case of the Cauchy-Schwarz Inequality (will denote as CS Inequality), which is \n",
    "\n",
    "    $|\\sum_{i=1}^{n}(x_i v_i)|^{2} \\leq (\\sum_{i=1}^{n}x_i^{2})(\\sum_{i=1}^{n}v_i^{2})$\n",
    "\n",
    "    We can see this by looking at the case where $x, v$ both only have nonnegative elements, and where each element of $v$, $v_i = 1$. In this manner the CS Inequality reduces to: \n",
    "\n",
    "    $ (\\lonenorm{x})^{2} \\leq (\\sum_{i=1}^{n}x_i^{2})(\\sum_{i=1}^{n}1) = n(\\sum_{i=1}^{n}|x_i|^{2}) $, \n",
    "\n",
    "    which is the inequality that we are trying to prove.\n",
    "\n",
    "\n",
    "5. $ \\sqrt{n}\\norm{x}_2 \\leq n\\norm{x}_{\\infty}$:\n",
    "\n",
    "    Squaring both sides and dividing by n, and referencing the result of the $l_{\\infty}$ norm by $x_{max}$ as before:\n",
    "\n",
    "    $ \\sum_{i=1}^{n} x_i^{2} \\leq n x_{max} = \\sum_{i=1}^{n} x_{max} $\n",
    "\n",
    "    This must hold as each element in the sum of the RHS is greater than those in the LHS, i.e $x_{max} \\geq x_i, \\forall i$.\n",
    "\n",
    "### Cardinality ###\n",
    "\n",
    "$card(x) \\geq \\dfrac{\\norm{x}_{1}^{2}}{\\norm{x}_{2}^{2}}$:\n",
    "\n",
    "For non-zero $x$.\n",
    "\n",
    "Denote $n'$ as the _effective_ dimensionality of the vector ${x}$, and $x'$ as the vector with reduced dimensionality. That is, if there are elements $x_i = 0$, these dimensions can effectively dropped from our consideration (the behaviour of the l1 and l2 norms are the same).  We can rewrite the equation as:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\norm{x'}_{1}^{2} &\\leq n'\\norm{x'}_{2}^{2} \\\\\n",
    "\\norm{x'}_{1} &\\leq n'\\norm{x'}_{2}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Note that the proof for this has already been done in the previous section (inequality number 4).\n",
    "\n",
    "The lower bound is obtained for $x$ with $n'= 1$, which will be the case for any one-hot vector (all elements except for one are zero'd out)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 ##\n",
    "\n",
    "Let:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "c_1 = \n",
    "\\begin{bmatrix}\n",
    " a_1 \\\\ \n",
    " b_1 \\\\\n",
    "\\end{bmatrix}, ..., \n",
    "c_k &= \n",
    "\\begin{bmatrix}\n",
    " a_k \\\\ \n",
    " b_k \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $a_1, ..., a_k$ or $b_1, ..., b_k$ are linearly independent, then $c_1, ... c_k$ must also be linearly independent.\n",
    "\n",
    "We will prove this for $a_1, ..., a_k$ but the same applies for $b_1, ..., b_k$. Assume towards a contradiction that $a_1, a_2$ are linearly independent and $c_1, ... c_k$ are linearly dependent. Then there is some set of weights $\\alpha$ and some $c_j$ for which \n",
    "\n",
    "$c_j = \\sum_{i=1, i \\neq j}^{k}\\alpha_i c_i$. \n",
    "\n",
    "If we look at just the $a$ constituent part of this equation, then this equates to:\n",
    "\n",
    "$a_j = \\sum_{i=1, i \\neq j}^{k}\\alpha_i a_i$\n",
    "\n",
    "This applies linear dependence for $a_1, ..., a_k$, which is clearly false; therefore there is a contradiction.\n",
    "\n",
    "### a) \n",
    "This follows from the above proof.\n",
    "\n",
    "### b)\n",
    "Also follows from the above proof. If $b_1, ..., b_k$ are linearly independent, then $c_1, ... c_k$ must also be linearly independent. Therefore we cannot conclude that they are linearly dependent only from knowing linearly dependence of $a_1, ..., a_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4 ##\n",
    "\n",
    "To show the orthogonality of unit vectors $x, y$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\innerprod{x - y}{x + y} \n",
    "&= \\innerprod{x}{x} + \\innerprod{x}{y} - \\innerprod{y}{x} - \\innerprod{y}{y} \\\\\n",
    "&= 1 + \\innerprod{x}{y} - \\innerprod{y}{x} - 1\\\\\n",
    "&= 1 - 1 + \\innerprod{x}{y} - \\innerprod{x}{y} \\\\\n",
    "&= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where line 1 is possible due to the linear property of the inner product and line 3 is possible due to the conjugate (conjugacy doesn't matter here as $x, y \\in \\mathbb{R}^{n}$) exchangeability of the terms.\n",
    "As the inner product is 0, the unit vectors $x, y$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.5 ##\n",
    "\n",
    "The result of running the Gram-Schmidt algorithm on this list of vectors is an orthonormal basis. \n",
    "\n",
    "To see this, let $u_i$ denote the one-hot vector for which only the $i^{th}$ dimension of the vector is equal to 1. Then running the Gram-Schmidt algorithm for the vectors $a_1, ..., a_k$ will form $q_1 = u_1, ..., q_k = u_k$.   \n",
    "\n",
    "We can prove this inductively. The inductive base will be shown for $a_2$:\n",
    "\n",
    "Note that $a_1$ is already normalized, so $q_1 = a_1$. Noting that the projection of $a_2$ onto $q_1$ is equal to 1 (they share 1 non-zero entry both of which are equal to 1), we proceed to form $q_2 = a_2 - (1)(a_1)$, which is also already normalized. It follows that $q_1 = u_1$ and $q_k = u_k$.\n",
    "\n",
    "For the inductive step, we will apply the algorithm on the $(k+1)^{th}$ vector. As each of the previous $k$ processed vectors $u_1, ..., u_k$ have a unit projection with $a_{k+1}$ and cover a different dimension, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{k+1} &= \n",
    "\\begin{bmatrix}\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "- 1\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "-, ..., - 1\n",
    "\\begin{bmatrix}\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix} \\\\\n",
    "&=u_{k+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.6 ##\n",
    "\n",
    "Positive-definiteness property of inner product spaces:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\innerprod{x}{x} &\\geq 0 \\\\\n",
    "\\innerprod{x}{x} &= 0 \\iff x = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because of positive-definiteness, there is a condition that each element $\\alpha_k > 0$.\n",
    "\n",
    "If this is not the case, it is possible to devise a one-hot vector $x$ such that only $x_k$ is non-zero for the element $\\alpha_k \\leq 0$, causing the $\\innerprod{x}{x} \\leq 0$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Q1.7 ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
