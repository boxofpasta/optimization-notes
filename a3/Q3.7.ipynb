{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.7 ##\n",
    "\n",
    "### a) (OptM 5.5 part 3) ###\n",
    "\n",
    "We will look at a case where $n$ is much larger than $m$, and $rank(\\tilde{M}) = m$. This is probably a common case as the number of unique words will typically be larger than the number of documents.\n",
    "\n",
    "Let the SVD of $\\tilde{M} = U \\Sigma V^T$.\n",
    "\n",
    "Consider first the product $U\\Sigma$. If we let the columns of $U$ represent the normalized, orthogonal term-vectors for each _concept_, then $U\\Sigma$ is the scaled version. That is, there are certain concepts for which all documents have more of in common, and thus these concepts will have larger scaling attached to them (The largest eigenvector-value pair corresponds to the direction of maximum variance). The idea is very similar to PCA, where the eigenvalues in $\\Sigma$ correspond to the directions of maximum variance (A difference is that in Latent Semantic Indexing our data is not centered).\n",
    "\n",
    "We can then examine the inclusion of $V^T$ to form the final $\\tilde{M}$. As the columns of $\\tilde{M}$ are the term-vectors for each document, we require a linear combination of the scaled concept term-vectors in $U\\Sigma$ to form the final document term-vectors. Hence, each column of $V^T$ is a concept scaling vector (the linear combination weights) for a certain document. Equivalently, the rows of $V^T$ describe the contribution of each individual concept term-vector into the final document term-vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) (OptM 5.5 part 4) ###\n",
    "\n",
    "To project any document vector (whether it be $\\tilde{q}$ or one of the columns in $\\tilde{M}$) onto the subspace spanned by $u_1, ..., u_k$:\n",
    "\n",
    "$q = U_k^T\\tilde{q}$\n",
    "\n",
    "If we want this projection to be normalized with respect to the variance of concept occurrences, which we typically do (similar to the idea of the inverse document frequency from Homework 1, which was for individual terms):\n",
    "\n",
    "$q_{normalized} = \\Sigma_k^{-1}U_k^T\\tilde{q}$\n",
    "\n",
    "The cosine similarity between any two projected, normalized terms is then:\n",
    "\n",
    "$ cos(\\theta) = \\dfrac{<q_1, q_2>}{\\sqrt{<q_1, q_1>}\\sqrt{<q_2, q_2>}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 singular values in descending order: \n",
      "[1.537 1.019 0.959 0.954 0.941 0.929 0.898 0.892 0.869 0.816]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.io import loadmat\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load the data.\n",
    "data_path = os.path.join('..', 'a1', 'PS01_dataSet', 'wordVecV.mat')\n",
    "data = loadmat(data_path)\n",
    "V = data['V'].astype(np.float64)\n",
    "num_docs = len(V[0])\n",
    "\n",
    "# Create the normalized term-by-document matrix M.\n",
    "M = np.clip(V, 0, 1.0)\n",
    "col_norms = np.sqrt(np.sum(M * M, axis=0))\n",
    "M /= col_norms\n",
    "\n",
    "# Do SVD.\n",
    "U, S, V_t = svd(M, full_matrices=True, compute_uv=True)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('The 10 singular values in descending order: ')\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n"
     ]
    }
   ],
   "source": [
    "def get_similarity(x, y):\n",
    "    cos_angle = np.dot(x, y) / (norm(x) * norm(y))\n",
    "    return cos_angle\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\" L2 norm. \"\"\"\n",
    "    return np.sqrt(np.sum(x * x))\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" L2 normalized. \"\"\"\n",
    "    return x / norm(x)\n",
    "\n",
    "def lsi_sim(U, S, q1, q2):\n",
    "    \"\"\"\n",
    "    Note that for LSI, the matrices should be approximations, \n",
    "    i.e the approximation has a lower rank and the matrices are cutoff.\n",
    "    :param U: The term-concept matrix, of shape (k, n).\n",
    "    :param S: The singular value array, of shape (k,).\n",
    "    :param q1: The first query, of shape (n,).\n",
    "    :param q2: The second query, of shape (n,).\n",
    "    \"\"\"\n",
    "    S_diag = np.diag(1.0 / S)\n",
    "    proj_q1 = np.matmul(S_diag, np.matmul(U.T, normalize(q1)))\n",
    "    proj_q2 = np.matmul(S_diag, np.matmul(U.T, normalize(q2)))\n",
    "    return get_similarity(proj_q1, proj_q2)\n",
    "\n",
    "max_rank = 9\n",
    "\n",
    "# Get the rank-cutoff matrices.\n",
    "U_k = U[:, :max_rank]\n",
    "S_k =S[:max_rank]\n",
    "\n",
    "# Compute pairwise distances, and keep track of the highest one.\n",
    "max_sim = 0.0\n",
    "max_sim_pair = None\n",
    "for i in range(num_docs):\n",
    "    for j in range(i + 1, num_docs):\n",
    "        sim = lsi_sim(U_k, S_k, M[:, i], M[:, j])\n",
    "        if sim > max_sim:\n",
    "            max_sim_pair = i + 1, j + 1\n",
    "            max_sim = sim\n",
    "            \n",
    "print('The pair with maximum similarity is: ' + str(max_sim_pair))\n",
    "print('The titles are: \"Barack Obama\" and \"George W. Bush\", respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 8: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 7: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 6: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 5: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 4: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 3: \n",
      "The pair with maximum similarity is: (9, 10)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 2: \n",
      "The pair with maximum similarity is: (1, 6)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n",
      "For k = 1: \n",
      "The pair with maximum similarity is: (1, 2)\n",
      "The titles are: \"Barack Obama\" and \"George W. Bush\", respectively\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for max_rank in range(8, 0, -1):\n",
    "    # Get the rank-cutoff matrices.\n",
    "    U_k = U[:, :max_rank]\n",
    "    S_k =S[:max_rank]\n",
    "\n",
    "    # Compute pairwise distances, and keep track of the highest one.\n",
    "    max_sim = 0.0\n",
    "    max_sim_pair = None\n",
    "    for i in range(num_docs):\n",
    "        for j in range(i + 1, num_docs):\n",
    "            sim = lsi_sim(U_k, S_k, M[:, i], M[:, j])\n",
    "            if sim > max_sim:\n",
    "                max_sim_pair = i + 1, j + 1\n",
    "                max_sim = sim\n",
    "    \n",
    "    print('For k = ' + str(max_rank) + ': ')\n",
    "    print('The pair with maximum similarity is: ' + str(max_sim_pair))\n",
    "    print('The titles are: \"Barack Obama\" and \"George W. Bush\", respectively')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest k that does not change our answer for part d) is k = 3. The pair of most similar documents for 2 is \"Barack Obama\" and \"George W. Bush\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
